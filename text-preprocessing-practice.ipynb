{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715}],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-15T05:17:46.748161Z","iopub.execute_input":"2024-07-15T05:17:46.748577Z","iopub.status.idle":"2024-07-15T05:17:46.758502Z","shell.execute_reply.started":"2024-07-15T05:17:46.748540Z","shell.execute_reply":"2024-07-15T05:17:46.757318Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# LowerCase","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-07-15T05:17:46.760347Z","iopub.execute_input":"2024-07-15T05:17:46.760688Z","iopub.status.idle":"2024-07-15T05:17:47.509623Z","shell.execute_reply.started":"2024-07-15T05:17:46.760662Z","shell.execute_reply":"2024-07-15T05:17:47.508315Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"df['review']=df['review'].str.lower()\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-15T05:17:47.510929Z","iopub.execute_input":"2024-07-15T05:17:47.511229Z","iopub.status.idle":"2024-07-15T05:17:47.734100Z","shell.execute_reply.started":"2024-07-15T05:17:47.511204Z","shell.execute_reply":"2024-07-15T05:17:47.732639Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"                                              review sentiment\n0  one of the other reviewers has mentioned that ...  positive\n1  a wonderful little production. <br /><br />the...  positive\n2  i thought this was a wonderful way to spend ti...  positive\n3  basically there's a family where a little boy ...  negative\n4  petter mattei's \"love in the time of money\" is...  positive","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>one of the other reviewers has mentioned that ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>a wonderful little production. &lt;br /&gt;&lt;br /&gt;the...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>i thought this was a wonderful way to spend ti...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>basically there's a family where a little boy ...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>petter mattei's \"love in the time of money\" is...</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Removing HTML tags","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import  re\ndef remove_html_tags(text):\n    pattern = re.compile('<.*?>')\n    return pattern.sub(r'',text)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T05:17:47.736845Z","iopub.execute_input":"2024-07-15T05:17:47.737297Z","iopub.status.idle":"2024-07-15T05:17:47.742670Z","shell.execute_reply.started":"2024-07-15T05:17:47.737256Z","shell.execute_reply":"2024-07-15T05:17:47.741503Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"text = df['review'][3]\ntext = remove_html_tags(text)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T05:45:29.138015Z","iopub.execute_input":"2024-07-15T05:45:29.138395Z","iopub.status.idle":"2024-07-15T05:45:29.143367Z","shell.execute_reply.started":"2024-07-15T05:45:29.138367Z","shell.execute_reply":"2024-07-15T05:45:29.142169Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"df['review']=df['review'].apply(remove_html_tags)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T05:18:15.992957Z","iopub.execute_input":"2024-07-15T05:18:15.993327Z","iopub.status.idle":"2024-07-15T05:18:16.088817Z","shell.execute_reply.started":"2024-07-15T05:18:15.993300Z","shell.execute_reply":"2024-07-15T05:18:16.087704Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Remove urls","metadata":{}},{"cell_type":"code","source":"def remove_url(text):\n    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    return pattern.sub(r'',text)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T05:17:47.973760Z","iopub.execute_input":"2024-07-15T05:17:47.974071Z","iopub.status.idle":"2024-07-15T05:17:47.980188Z","shell.execute_reply.started":"2024-07-15T05:17:47.974042Z","shell.execute_reply":"2024-07-15T05:17:47.978306Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"df['review'] = df['review'].apply(remove_url)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T05:18:35.353219Z","iopub.execute_input":"2024-07-15T05:18:35.353683Z","iopub.status.idle":"2024-07-15T05:18:36.039830Z","shell.execute_reply.started":"2024-07-15T05:18:35.353648Z","shell.execute_reply":"2024-07-15T05:18:36.038726Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Removing Punctuation","metadata":{}},{"cell_type":"code","source":"import string\nexclude = string.punctuation\nprint(exclude)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T05:41:47.843363Z","iopub.execute_input":"2024-07-15T05:41:47.844330Z","iopub.status.idle":"2024-07-15T05:41:47.849994Z","shell.execute_reply.started":"2024-07-15T05:41:47.844287Z","shell.execute_reply":"2024-07-15T05:41:47.848964Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n","output_type":"stream"}]},{"cell_type":"code","source":"def remove_punc(text):\n    return text.translate(str.maketrans('','',exclude))","metadata":{"execution":{"iopub.status.busy":"2024-07-15T05:41:58.074440Z","iopub.execute_input":"2024-07-15T05:41:58.074870Z","iopub.status.idle":"2024-07-15T05:41:58.080476Z","shell.execute_reply.started":"2024-07-15T05:41:58.074811Z","shell.execute_reply":"2024-07-15T05:41:58.079345Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"df['review'] = df['review'].apply(remove_punc)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T05:45:18.553573Z","iopub.execute_input":"2024-07-15T05:45:18.553986Z","iopub.status.idle":"2024-07-15T05:45:19.815201Z","shell.execute_reply.started":"2024-07-15T05:45:18.553955Z","shell.execute_reply":"2024-07-15T05:45:19.814079Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"# Converting short forms","metadata":{}},{"cell_type":"markdown","source":"We will make a dictionary relating short and its long forms ","metadata":{}},{"cell_type":"code","source":"def chat_conversion(text):\n    new_text = []\n    for w in text.split():\n        if w.upper() in chat_words:\n            new_text.append(chat_words[w.upper()])\n        else:\n            new_text.append(w)\n    return \" \".join(new_text)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Correcting spelling mistakes","metadata":{}},{"cell_type":"code","source":"from textblob import TextBlob\n\ntext = 'helloo theree whet\\'s gooing oon'\ntextBlb = TextBlob(text)\ntextBlb.correct().string","metadata":{"execution":{"iopub.status.busy":"2024-07-15T05:59:27.913381Z","iopub.execute_input":"2024-07-15T05:59:27.914645Z","iopub.status.idle":"2024-07-15T05:59:27.923995Z","shell.execute_reply.started":"2024-07-15T05:59:27.914603Z","shell.execute_reply":"2024-07-15T05:59:27.922798Z"},"trusted":true},"execution_count":51,"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"\"hello there what's going on\""},"metadata":{}}]},{"cell_type":"markdown","source":"# Removing stoping words","metadata":{}},{"cell_type":"markdown","source":"It is not always necessary, but in applicatons like sentiment analysis its useful (NLTK have tools for that). ","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\nstopwords.words('english')","metadata":{"execution":{"iopub.status.busy":"2024-07-15T06:11:56.043213Z","iopub.execute_input":"2024-07-15T06:11:56.043670Z","iopub.status.idle":"2024-07-15T06:11:56.056369Z","shell.execute_reply.started":"2024-07-15T06:11:56.043635Z","shell.execute_reply":"2024-07-15T06:11:56.055002Z"},"trusted":true},"execution_count":56,"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"['i',\n 'me',\n 'my',\n 'myself',\n 'we',\n 'our',\n 'ours',\n 'ourselves',\n 'you',\n \"you're\",\n \"you've\",\n \"you'll\",\n \"you'd\",\n 'your',\n 'yours',\n 'yourself',\n 'yourselves',\n 'he',\n 'him',\n 'his',\n 'himself',\n 'she',\n \"she's\",\n 'her',\n 'hers',\n 'herself',\n 'it',\n \"it's\",\n 'its',\n 'itself',\n 'they',\n 'them',\n 'their',\n 'theirs',\n 'themselves',\n 'what',\n 'which',\n 'who',\n 'whom',\n 'this',\n 'that',\n \"that'll\",\n 'these',\n 'those',\n 'am',\n 'is',\n 'are',\n 'was',\n 'were',\n 'be',\n 'been',\n 'being',\n 'have',\n 'has',\n 'had',\n 'having',\n 'do',\n 'does',\n 'did',\n 'doing',\n 'a',\n 'an',\n 'the',\n 'and',\n 'but',\n 'if',\n 'or',\n 'because',\n 'as',\n 'until',\n 'while',\n 'of',\n 'at',\n 'by',\n 'for',\n 'with',\n 'about',\n 'against',\n 'between',\n 'into',\n 'through',\n 'during',\n 'before',\n 'after',\n 'above',\n 'below',\n 'to',\n 'from',\n 'up',\n 'down',\n 'in',\n 'out',\n 'on',\n 'off',\n 'over',\n 'under',\n 'again',\n 'further',\n 'then',\n 'once',\n 'here',\n 'there',\n 'when',\n 'where',\n 'why',\n 'how',\n 'all',\n 'any',\n 'both',\n 'each',\n 'few',\n 'more',\n 'most',\n 'other',\n 'some',\n 'such',\n 'no',\n 'nor',\n 'not',\n 'only',\n 'own',\n 'same',\n 'so',\n 'than',\n 'too',\n 'very',\n 's',\n 't',\n 'can',\n 'will',\n 'just',\n 'don',\n \"don't\",\n 'should',\n \"should've\",\n 'now',\n 'd',\n 'll',\n 'm',\n 'o',\n 're',\n 've',\n 'y',\n 'ain',\n 'aren',\n \"aren't\",\n 'couldn',\n \"couldn't\",\n 'didn',\n \"didn't\",\n 'doesn',\n \"doesn't\",\n 'hadn',\n \"hadn't\",\n 'hasn',\n \"hasn't\",\n 'haven',\n \"haven't\",\n 'isn',\n \"isn't\",\n 'ma',\n 'mightn',\n \"mightn't\",\n 'mustn',\n \"mustn't\",\n 'needn',\n \"needn't\",\n 'shan',\n \"shan't\",\n 'shouldn',\n \"shouldn't\",\n 'wasn',\n \"wasn't\",\n 'weren',\n \"weren't\",\n 'won',\n \"won't\",\n 'wouldn',\n \"wouldn't\"]"},"metadata":{}}]},{"cell_type":"code","source":"from nltk.corpus import stopwords\ndef remove_stopwords(text):\n    new_text = []\n    for word in text.split():\n        if word in stopwords.words('english'):\n            new_text.append('')\n        else:\n            new_text.append(word)\n    x = new_text[:]\n    new_text.clear()\n    return \" \".join(x)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T06:33:37.673896Z","iopub.execute_input":"2024-07-15T06:33:37.674972Z","iopub.status.idle":"2024-07-15T06:33:37.680800Z","shell.execute_reply.started":"2024-07-15T06:33:37.674930Z","shell.execute_reply":"2024-07-15T06:33:37.679686Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"df['review'] = df['review'].apply(remove_stopwords)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T06:33:43.802435Z","iopub.execute_input":"2024-07-15T06:33:43.802841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Handling Emojis","metadata":{}},{"cell_type":"markdown","source":"You can  eiher remove emoji or convert it to its corresponding word","metadata":{}},{"cell_type":"code","source":"# Removing\nimport re\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T12:18:48.337843Z","iopub.execute_input":"2024-07-15T12:18:48.338236Z","iopub.status.idle":"2024-07-15T12:18:48.369668Z","shell.execute_reply.started":"2024-07-15T12:18:48.338186Z","shell.execute_reply":"2024-07-15T12:18:48.368752Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import emoji\nprint(emoji.demojize('Python is ðŸ”¥'))","metadata":{"execution":{"iopub.status.busy":"2024-07-15T12:20:23.631528Z","iopub.execute_input":"2024-07-15T12:20:23.631982Z","iopub.status.idle":"2024-07-15T12:20:23.702232Z","shell.execute_reply.started":"2024-07-15T12:20:23.631951Z","shell.execute_reply":"2024-07-15T12:20:23.701169Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Python is :fire:\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Tokenisation","metadata":{}},{"cell_type":"markdown","source":"1) Split 2) Regex","metadata":{}},{"cell_type":"markdown","source":"## 3) NLTK","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize,sent_tokenize","metadata":{"execution":{"iopub.status.busy":"2024-07-15T12:42:59.973470Z","iopub.execute_input":"2024-07-15T12:42:59.974241Z","iopub.status.idle":"2024-07-15T12:43:01.240869Z","shell.execute_reply.started":"2024-07-15T12:42:59.974197Z","shell.execute_reply":"2024-07-15T12:43:01.239680Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"sent1 = 'I am going to visit delhi!'\nword_tokenize(sent1)\n\ntext = \"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry? \nLorem Ipsum has been the industry's standard dummy text ever since the 1500s, \nwhen an unknown printer took a galley of type and scrambled it to make a type specimen book.\"\"\"\n\nsent_tokenize(text)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T12:44:09.772728Z","iopub.execute_input":"2024-07-15T12:44:09.773912Z","iopub.status.idle":"2024-07-15T12:44:09.783528Z","shell.execute_reply.started":"2024-07-15T12:44:09.773876Z","shell.execute_reply":"2024-07-15T12:44:09.782268Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"['Lorem Ipsum is simply dummy text of the printing and typesetting industry?',\n \"Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, \\nwhen an unknown printer took a galley of type and scrambled it to make a type specimen book.\"]"},"metadata":{}}]},{"cell_type":"markdown","source":"## 4) SPACY","metadata":{}},{"cell_type":"code","source":"import spacy\nnlp = spacy.load('en_core_web_sm')\n\ndoc = nlp(sent1)\n\nfor token in doc:\n    print(token)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T12:45:56.580565Z","iopub.execute_input":"2024-07-15T12:45:56.580996Z","iopub.status.idle":"2024-07-15T12:46:01.146087Z","shell.execute_reply.started":"2024-07-15T12:45:56.580965Z","shell.execute_reply":"2024-07-15T12:46:01.144923Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"I\nam\ngoing\nto\nvisit\ndelhi\n!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Stemming and Lemmitization","metadata":{}},{"cell_type":"markdown","source":"### stemming is fast root word m convert krta jo jaruri nahi us language m ho..lemmitization m similar dictionary root word m convert krta h (searching hoti h)","metadata":{}},{"cell_type":"code","source":"#stemming\nfrom nltk.stem.porter import PorterStemmer\n\nps = PorterStemmer()\ndef stem_words(text):\n    return \" \".join([ps.stem(word) for word in text.split()])\n\nsample = \"walk walks walking walked\"\nstem_words(sample)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T13:08:43.572959Z","iopub.execute_input":"2024-07-15T13:08:43.574294Z","iopub.status.idle":"2024-07-15T13:08:43.582990Z","shell.execute_reply.started":"2024-07-15T13:08:43.574255Z","shell.execute_reply":"2024-07-15T13:08:43.581840Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"'walk walk walk walk'"},"metadata":{}}]},{"cell_type":"code","source":"#Lemmitization\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\n\n# Ensure necessary resources are downloaded\nnltk.download('punkt')\nnltk.download('wordnet')\n\n# Initialize the WordNet lemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\n\n# Define the input sentence\nsentence = \"He was running and eating at the same time. He has a bad habit of swimming after playing long hours in the Sun.\"\n\n# Define punctuation to remove\npunctuations = \"?:!.,;\"\n\n# Tokenize the sentence\nsentence_words = nltk.word_tokenize(sentence)\n\n# Remove punctuation from the tokenized words\nsentence_words = [word for word in sentence_words if word not in punctuations]\n\n# Print the header\nprint(\"{0:20}{1:20}\".format(\"Word\", \"Lemma\"))\n\n# Lemmatize each word and print\nfor word in sentence_words:\n    print(\"{0:20}{1:20}\".format(word, wordnet_lemmatizer.lemmatize(word, pos='v')))\n","metadata":{"execution":{"iopub.status.busy":"2024-07-15T13:11:58.615591Z","iopub.execute_input":"2024-07-15T13:11:58.616429Z","iopub.status.idle":"2024-07-15T13:12:38.857745Z","shell.execute_reply.started":"2024-07-15T13:11:58.616395Z","shell.execute_reply":"2024-07-15T13:12:38.856104Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"[nltk_data] Error loading punkt: <urlopen error [Errno -3] Temporary\n[nltk_data]     failure in name resolution>\n[nltk_data] Error loading wordnet: <urlopen error [Errno -3] Temporary\n[nltk_data]     failure in name resolution>\nWord                Lemma               \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/corpus/util.py:80\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzip_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m e\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/data.py:653\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    652\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (sep, msg, sep)\n\u001b[0;32m--> 653\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/wordnet.zip/wordnet/.zip/' not found.  Please\n  use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Lemmatize each word and print\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m sentence_words:\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0:20}\u001b[39;00m\u001b[38;5;132;01m{1:20}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(word, \u001b[43mwordnet_lemmatizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/stem/wordnet.py:40\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize\u001b[39m(\u001b[38;5;28mself\u001b[39m, word, pos\u001b[38;5;241m=\u001b[39mNOUN):\n\u001b[0;32m---> 40\u001b[0m     lemmas \u001b[38;5;241m=\u001b[39m \u001b[43mwordnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_morphy\u001b[49m(word, pos)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(lemmas, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m lemmas \u001b[38;5;28;01melse\u001b[39;00m word\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/corpus/util.py:116\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m: root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir, zip_name))\n\u001b[0;32m---> 81\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/corpus/util.py:78\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m: root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir, zip_name))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/data.py:653\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    651\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    652\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (sep, msg, sep)\n\u001b[0;32m--> 653\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/wordnet' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************"],"ename":"LookupError","evalue":"\n**********************************************************************\n  Resource 'corpora/wordnet' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************","output_type":"error"}]}]}